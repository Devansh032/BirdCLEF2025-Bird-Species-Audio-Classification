{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12260026,"sourceType":"datasetVersion","datasetId":7725603}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport os\nimport numpy as np\nimport torchvision.transforms.functional as TF\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torchvision.models as models\nfrom transformers import ViTForImageClassification\n\n!pip install librosa --quiet\n\nimport os\nimport librosa\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport os\n\n","metadata":{"id":"Z_JQVQih9YzO","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:13:47.850813Z","iopub.execute_input":"2025-06-23T20:13:47.851483Z","iopub.status.idle":"2025-06-23T20:13:50.927314Z","shell.execute_reply.started":"2025-06-23T20:13:47.851455Z","shell.execute_reply":"2025-06-23T20:13:50.926491Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# List all datasets mounted in Kaggle input directory\nprint(\"Available datasets under /kaggle/input:\")\nprint(os.listdir(\"/kaggle/input/bird-dataset/dataset\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:13:50.929071Z","iopub.execute_input":"2025-06-23T20:13:50.929364Z","iopub.status.idle":"2025-06-23T20:13:50.935657Z","shell.execute_reply.started":"2025-06-23T20:13:50.929334Z","shell.execute_reply":"2025-06-23T20:13:50.935063Z"}},"outputs":[{"name":"stdout","text":"Available datasets under /kaggle/input:\n['greani1', 'thbeup1', 'yebsee1', 'speowl1', 'strowl1', 'blbwre1', 'rufmot1', 'anhing', 'babwar', 'srwswa1', 'eardov1', 'recwoo1', 'gybmar', 'y00678', 'chfmac1', 'neocor', 'yectyr1', 'thlsch3', 'gohman1', 'whfant1', 'purgal2', 'ywcpar', 'crcwoo1', 'whbman1', 'plbwoo1', 'bicwre1', 'ragmac1', 'secfly1', 'gretin1', 'colcha1', 'ruther1', '65448', 'whbant1', 'cocwoo1', 'rugdov', 'cargra1', 'brtpar1', '41663', 'snoegr', 'baymac', 'yecspi2', 'watjac1', 'bkcdon', 'greibi1', 'rtlhum', 'mastit1', 'pavpig2', 'amakin1', 'leagre', 'blcjay1', 'cattyr', 'grbhaw1', 'grnkin']\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\ndef split_dataset(source_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n    assert train_ratio + val_ratio + test_ratio == 1.0\n    os.makedirs(output_dir, exist_ok=True)\n\n    for species in os.listdir(source_dir):\n        species_path = os.path.join(source_dir, species)\n        if not os.path.isdir(species_path):\n            continue\n\n        files = os.listdir(species_path)\n        random.shuffle(files)\n\n        n = len(files)\n        n_train = int(train_ratio * n)\n        n_val = int(val_ratio * n)\n        n_test = n - n_train - n_val\n\n        splits = {\n            \"train\": files[:n_train],\n            \"val\": files[n_train:n_train + n_val],\n            \"test\": files[n_train + n_val:]\n        }\n\n        for split_name, split_files in splits.items():\n            split_dir = os.path.join(output_dir, split_name, species)\n            os.makedirs(split_dir, exist_ok=True)\n            for file in split_files:\n                src = os.path.join(species_path, file)\n                dst = os.path.join(split_dir, file)\n                shutil.copy2(src, dst)\n\n        print(f\"{species}: {n_train} train, {n_val} val, {n_test} test\")\n\n# Run the function with Kaggle paths\nsplit_dataset(\n    source_dir=\"/kaggle/input/bird-dataset/dataset\",\n    output_dir=\"/kaggle/working/split-bird-dataset\",\n    train_ratio=0.7,\n    val_ratio=0.15,\n    test_ratio=0.15\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:13:50.936461Z","iopub.execute_input":"2025-06-23T20:13:50.936892Z","iopub.status.idle":"2025-06-23T20:14:21.982221Z","shell.execute_reply.started":"2025-06-23T20:13:50.936876Z","shell.execute_reply":"2025-06-23T20:14:21.981466Z"}},"outputs":[{"name":"stdout","text":"greani1: 39 train, 8 val, 9 test\nthbeup1: 53 train, 11 val, 12 test\nyebsee1: 68 train, 14 val, 16 test\nspeowl1: 56 train, 12 val, 13 test\nstrowl1: 54 train, 11 val, 13 test\nblbwre1: 46 train, 9 val, 11 test\nrufmot1: 37 train, 8 val, 9 test\nanhing: 44 train, 9 val, 11 test\nbabwar: 58 train, 12 val, 13 test\nsrwswa1: 55 train, 11 val, 13 test\neardov1: 49 train, 10 val, 12 test\nrecwoo1: 48 train, 10 val, 11 test\ngybmar: 77 train, 16 val, 18 test\ny00678: 81 train, 17 val, 18 test\nchfmac1: 57 train, 12 val, 13 test\nneocor: 44 train, 9 val, 10 test\nyectyr1: 46 train, 10 val, 11 test\nthlsch3: 36 train, 7 val, 9 test\ngohman1: 47 train, 10 val, 11 test\nwhfant1: 45 train, 9 val, 11 test\npurgal2: 77 train, 16 val, 17 test\nywcpar: 51 train, 11 val, 12 test\ncrcwoo1: 49 train, 10 val, 11 test\nwhbman1: 83 train, 17 val, 19 test\nplbwoo1: 78 train, 16 val, 18 test\nbicwre1: 47 train, 10 val, 11 test\nragmac1: 42 train, 9 val, 9 test\nsecfly1: 82 train, 17 val, 19 test\ngretin1: 39 train, 8 val, 9 test\ncolcha1: 45 train, 9 val, 11 test\nruther1: 38 train, 8 val, 9 test\n65448: 44 train, 9 val, 11 test\nwhbant1: 42 train, 9 val, 9 test\ncocwoo1: 62 train, 13 val, 14 test\nrugdov: 67 train, 14 val, 15 test\ncargra1: 40 train, 8 val, 10 test\nbrtpar1: 36 train, 7 val, 9 test\n41663: 41 train, 8 val, 10 test\nsnoegr: 54 train, 11 val, 13 test\nbaymac: 67 train, 14 val, 15 test\nyecspi2: 81 train, 17 val, 19 test\nwatjac1: 70 train, 15 val, 15 test\nbkcdon: 68 train, 14 val, 16 test\ngreibi1: 49 train, 10 val, 12 test\nrtlhum: 59 train, 12 val, 14 test\nmastit1: 51 train, 11 val, 12 test\npavpig2: 61 train, 13 val, 14 test\namakin1: 53 train, 11 val, 12 test\nleagre: 51 train, 11 val, 12 test\nblcjay1: 36 train, 7 val, 9 test\ncattyr: 46 train, 9 val, 11 test\ngrbhaw1: 53 train, 11 val, 12 test\ngrnkin: 67 train, 14 val, 15 test\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def audio_to_mel(file_path, sr=48000, duration=3, n_mels=64, hop_length=128, n_fft=512):\n    y, _ = librosa.load(file_path, sr=sr)\n    y = librosa.util.fix_length(y, size=sr * duration)  # pad or trim to 3 sec\n    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft,\n                                         hop_length=hop_length, n_mels=n_mels, fmax=15000)\n    mel_db = librosa.power_to_db(mel, ref=np.max)\n    return mel_db  # Shape should be ~64 x 384\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:14:21.984088Z","iopub.execute_input":"2025-06-23T20:14:21.984311Z","iopub.status.idle":"2025-06-23T20:14:21.989177Z","shell.execute_reply.started":"2025-06-23T20:14:21.984295Z","shell.execute_reply":"2025-06-23T20:14:21.988386Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def generate_all_mels(input_dir, output_dir):\n    for split in ['train', 'val', 'test']:\n        input_split = os.path.join(input_dir, split)\n        output_split = os.path.join(output_dir, split)\n        os.makedirs(output_split, exist_ok=True)\n\n        for species in tqdm(os.listdir(input_split), desc=f\"Processing {split}\"):\n            input_species_path = os.path.join(input_split, species)\n            output_species_path = os.path.join(output_split, species)\n            os.makedirs(output_species_path, exist_ok=True)\n\n            for file in os.listdir(input_species_path):\n                if not file.endswith(\".ogg\"):\n                    continue\n\n                input_file_path = os.path.join(input_species_path, file)\n                output_file_path = os.path.join(output_species_path, file.replace(\".ogg\", \".npy\"))\n\n                try:\n                    mel = audio_to_mel(input_file_path)\n                    np.save(output_file_path, mel)\n                except Exception as e:\n                    print(f\"Error processing {file}: {e}\")\n\ngenerate_all_mels(\"/kaggle/working/split-bird-dataset\", \"/kaggle/working/MelSpectrograms\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:14:21.989860Z","iopub.execute_input":"2025-06-23T20:14:21.990039Z","iopub.status.idle":"2025-06-23T20:17:01.525175Z","shell.execute_reply.started":"2025-06-23T20:14:21.990026Z","shell.execute_reply":"2025-06-23T20:17:01.524502Z"}},"outputs":[{"name":"stderr","text":"Processing train: 100%|██████████| 53/53 [01:51<00:00,  2.10s/it]\nProcessing val: 100%|██████████| 53/53 [00:22<00:00,  2.33it/s]\nProcessing test: 100%|██████████| 53/53 [00:25<00:00,  2.06it/s]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"base_dir = \"/kaggle/working/MelSpectrograms\"\n\ntrain_dir = f'{base_dir}/train'\nval_dir   = f'{base_dir}/val'\ntest_dir  = f'{base_dir}/test'\n","metadata":{"id":"KUNWuYqzUM3q","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:01.526074Z","iopub.execute_input":"2025-06-23T20:17:01.526699Z","iopub.status.idle":"2025-06-23T20:17:01.530204Z","shell.execute_reply.started":"2025-06-23T20:17:01.526680Z","shell.execute_reply":"2025-06-23T20:17:01.529514Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\nclass ResizeNormalize:\n    def __init__(self, size=(224, 224)):\n        self.size = size\n\n    def __call__(self, tensor):\n        tensor = TF.resize(tensor, self.size)\n        tensor = TF.normalize(tensor, mean=[0.5]*3, std=[0.5]*3)\n        return tensor\n\nclass MelSpectrogramDataset(Dataset):\n    def __init__(self, folder_path, class_map, transform=None):\n        self.folder_path = folder_path\n        self.transform = transform\n        self.class_map = class_map  # dict: class_name -> label index\n\n        self.files = []\n        for label_name in os.listdir(folder_path):\n            class_folder = os.path.join(folder_path, label_name)\n            for fname in os.listdir(class_folder):\n                if fname.endswith('.npy'):\n                    self.files.append((os.path.join(class_folder, fname), self.class_map[label_name]))\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        path, label = self.files[idx]\n        mel = np.load(path)  # shape: [H, W]\n        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).repeat(3, 1, 1)  # [3, H, W]\n        if self.transform:\n            mel = self.transform(mel)\n        return mel, label\n","metadata":{"id":"O7OX7cN89Plh","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:01.530853Z","iopub.execute_input":"2025-06-23T20:17:01.531298Z","iopub.status.idle":"2025-06-23T20:17:01.546379Z","shell.execute_reply.started":"2025-06-23T20:17:01.531281Z","shell.execute_reply":"2025-06-23T20:17:01.545706Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Get class labels from subfolders of 'train'\nclass_names = sorted(os.listdir(train_dir))\nclass_map = {cls: idx for idx, cls in enumerate(class_names)}\n\nprint(\"Class Map:\", class_map)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PbYgncVy9ShO","outputId":"e26f9296-3e9a-4fa5-9dae-b8a196fdfcd5","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:01.546983Z","iopub.execute_input":"2025-06-23T20:17:01.547174Z","iopub.status.idle":"2025-06-23T20:17:01.565234Z","shell.execute_reply.started":"2025-06-23T20:17:01.547159Z","shell.execute_reply":"2025-06-23T20:17:01.564744Z"}},"outputs":[{"name":"stdout","text":"Class Map: {'41663': 0, '65448': 1, 'amakin1': 2, 'anhing': 3, 'babwar': 4, 'baymac': 5, 'bicwre1': 6, 'bkcdon': 7, 'blbwre1': 8, 'blcjay1': 9, 'brtpar1': 10, 'cargra1': 11, 'cattyr': 12, 'chfmac1': 13, 'cocwoo1': 14, 'colcha1': 15, 'crcwoo1': 16, 'eardov1': 17, 'gohman1': 18, 'grbhaw1': 19, 'greani1': 20, 'greibi1': 21, 'gretin1': 22, 'grnkin': 23, 'gybmar': 24, 'leagre': 25, 'mastit1': 26, 'neocor': 27, 'pavpig2': 28, 'plbwoo1': 29, 'purgal2': 30, 'ragmac1': 31, 'recwoo1': 32, 'rtlhum': 33, 'rufmot1': 34, 'rugdov': 35, 'ruther1': 36, 'secfly1': 37, 'snoegr': 38, 'speowl1': 39, 'srwswa1': 40, 'strowl1': 41, 'thbeup1': 42, 'thlsch3': 43, 'watjac1': 44, 'whbant1': 45, 'whbman1': 46, 'whfant1': 47, 'y00678': 48, 'yebsee1': 49, 'yecspi2': 50, 'yectyr1': 51, 'ywcpar': 52}\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"\ntransform = ResizeNormalize()\n\ntrain_data = MelSpectrogramDataset(train_dir, class_map, transform)\nval_data   = MelSpectrogramDataset(val_dir, class_map, transform)\ntest_data  = MelSpectrogramDataset(test_dir, class_map, transform)\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_data, batch_size=32)\ntest_loader  = DataLoader(test_data, batch_size=32)\n","metadata":{"id":"1FQCBREd9UQX","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:01.565819Z","iopub.execute_input":"2025-06-23T20:17:01.566064Z","iopub.status.idle":"2025-06-23T20:17:01.589826Z","shell.execute_reply.started":"2025-06-23T20:17:01.566040Z","shell.execute_reply":"2025-06-23T20:17:01.589192Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\n\nnum_classes = len(class_map)\n\n# ResNet-50\nfrom torchvision.models import ResNet50_Weights\nresnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\nresnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n\n# DenseNet-161\nfrom torchvision.models import DenseNet161_Weights\ndensenet161 = models.densenet161(weights=DenseNet161_Weights.DEFAULT)\ndensenet161.classifier = nn.Linear(densenet161.classifier.in_features, num_classes)\n\n# ViT-B/16\nvit = ViTForImageClassification.from_pretrained(\n    \"google/vit-base-patch16-224-in21k\",\n    num_labels=num_classes,\n    ignore_mismatched_sizes=True  # required if using custom classes\n)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IagCGXNd-FCa","outputId":"2506526a-c69d-4103-99db-967296bbff26","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:01.592082Z","iopub.execute_input":"2025-06-23T20:17:01.592272Z","iopub.status.idle":"2025-06-23T20:17:07.064344Z","shell.execute_reply.started":"2025-06-23T20:17:01.592258Z","shell.execute_reply":"2025-06-23T20:17:07.063779Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 164MB/s] \nDownloading: \"https://download.pytorch.org/models/densenet161-8d451a50.pth\" to /root/.cache/torch/hub/checkpoints/densenet161-8d451a50.pth\n100%|██████████| 110M/110M [00:01<00:00, 87.3MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6268be15ba1446e48b4baa2d72d4fd3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"795e498a1d224f33a2bace2dd85d3d49"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device) #cpu\nresnet50 = resnet50.to(device)\ndensenet161 = densenet161.to(device)\nvit = vit.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXve0DZe-G7k","outputId":"885ce700-6ed8-4c3a-818e-bb473d4dc645","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:07.065065Z","iopub.execute_input":"2025-06-23T20:17:07.065317Z","iopub.status.idle":"2025-06-23T20:17:07.482843Z","shell.execute_reply.started":"2025-06-23T20:17:07.065301Z","shell.execute_reply":"2025-06-23T20:17:07.482053Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n\ndef train_model(model, train_loader, val_loader, epochs=10, save_path=\"best_model.pth\", lr=1e-4, patience=3):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    best_val_acc = 0.0\n    epochs_no_improve = 0\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss, total_correct = 0, 0\n\n        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs) if not isinstance(model, ViTForImageClassification) else model(inputs).logits\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item() * inputs.size(0)\n            total_correct += (outputs.argmax(1) == labels).sum().item()\n\n        train_acc = total_correct / len(train_loader.dataset)\n        avg_loss = total_loss / len(train_loader.dataset)\n        print(f\"Train Loss: {avg_loss:.4f} | Accuracy: {train_acc:.4f}\")\n\n        # Validate\n        val_acc = evaluate_model(model, val_loader)\n\n        # Early Stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), save_path)\n            print(f\"✅ New best model saved with Val Accuracy: {val_acc:.4f}\")\n        else:\n            epochs_no_improve += 1\n            print(f\"⚠️ No improvement for {epochs_no_improve} epoch(s).\")\n\n        if epochs_no_improve >= patience:\n            print(\"🛑 Early stopping triggered.\")\n            break\n","metadata":{"id":"o5RuSPXz-KOb","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:07.483749Z","iopub.execute_input":"2025-06-23T20:17:07.484372Z","iopub.status.idle":"2025-06-23T20:17:07.491904Z","shell.execute_reply.started":"2025-06-23T20:17:07.484344Z","shell.execute_reply":"2025-06-23T20:17:07.491165Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def evaluate_model(model, data_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    device = next(model.parameters()).device\n\n    with torch.no_grad():\n        for X, y in data_loader:\n            X, y = X.to(device), y.to(device)\n            outputs = model(X) if not isinstance(model, ViTForImageClassification) else model(X).logits\n            preds = outputs.argmax(1)\n            correct += (preds == y).sum().item()\n            total += y.size(0)\n\n    acc = correct / total if total > 0 else 0.0\n    print(f\"Validation Accuracy: {acc:.4f}\")\n    return acc\n","metadata":{"id":"SW9nMwPC-MIQ","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:07.492620Z","iopub.execute_input":"2025-06-23T20:17:07.492859Z","iopub.status.idle":"2025-06-23T20:17:07.512612Z","shell.execute_reply.started":"2025-06-23T20:17:07.492835Z","shell.execute_reply":"2025-06-23T20:17:07.511945Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"train_model(resnet50, train_loader, val_loader, epochs=15,save_path=\"best_model_resnet50\")","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"YkEfDBDR-QEh","outputId":"91a4cd2d-986a-468e-ae80-73f827cec778","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:17:07.513363Z","iopub.execute_input":"2025-06-23T20:17:07.513638Z","iopub.status.idle":"2025-06-23T20:20:09.006500Z","shell.execute_reply.started":"2025-06-23T20:17:07.513622Z","shell.execute_reply":"2025-06-23T20:20:09.005849Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 97/97 [00:33<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.8792 | Accuracy: 0.0686\nValidation Accuracy: 0.1553\n✅ New best model saved with Val Accuracy: 0.1553\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 97/97 [00:32<00:00,  3.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.9395 | Accuracy: 0.3108\nValidation Accuracy: 0.3967\n✅ New best model saved with Val Accuracy: 0.3967\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 97/97 [00:32<00:00,  2.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8229 | Accuracy: 0.5618\nValidation Accuracy: 0.5395\n✅ New best model saved with Val Accuracy: 0.5395\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 97/97 [00:33<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0411 | Accuracy: 0.7704\nValidation Accuracy: 0.5381\n⚠️ No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 97/97 [00:34<00:00,  2.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5347 | Accuracy: 0.8952\nValidation Accuracy: 0.5562\n✅ New best model saved with Val Accuracy: 0.5562\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"evaluate_model(resnet50, test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:20:09.007544Z","iopub.execute_input":"2025-06-23T20:20:09.007781Z","iopub.status.idle":"2025-06-23T20:20:12.199229Z","shell.execute_reply.started":"2025-06-23T20:20:09.007765Z","shell.execute_reply":"2025-06-23T20:20:12.198530Z"}},"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.5620\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.561963190184049"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"train_model(densenet161, train_loader, val_loader, epochs=10,save_path = \"best_model_densenet161\")","metadata":{"id":"yLkXMMSwCyQg","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:20:12.200185Z","iopub.execute_input":"2025-06-23T20:20:12.200521Z","iopub.status.idle":"2025-06-23T20:33:38.984618Z","shell.execute_reply.started":"2025-06-23T20:20:12.200497Z","shell.execute_reply":"2025-06-23T20:33:38.983932Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 97/97 [01:11<00:00,  1.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.5134 | Accuracy: 0.1746\nValidation Accuracy: 0.3537\n✅ New best model saved with Val Accuracy: 0.3537\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 97/97 [01:14<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.2869 | Accuracy: 0.4932\nValidation Accuracy: 0.5479\n✅ New best model saved with Val Accuracy: 0.5479\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 97/97 [01:14<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4421 | Accuracy: 0.7063\nValidation Accuracy: 0.6241\n✅ New best model saved with Val Accuracy: 0.6241\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 97/97 [01:15<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8118 | Accuracy: 0.8687\nValidation Accuracy: 0.6477\n✅ New best model saved with Val Accuracy: 0.6477\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 97/97 [01:14<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3577 | Accuracy: 0.9576\nValidation Accuracy: 0.6824\n✅ New best model saved with Val Accuracy: 0.6824\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 97/97 [01:14<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1566 | Accuracy: 0.9897\nValidation Accuracy: 0.7115\n✅ New best model saved with Val Accuracy: 0.7115\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 97/97 [01:14<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0597 | Accuracy: 0.9994\nValidation Accuracy: 0.7129\n✅ New best model saved with Val Accuracy: 0.7129\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 97/97 [01:14<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0300 | Accuracy: 1.0000\nValidation Accuracy: 0.7129\n⚠️ No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 97/97 [01:14<00:00,  1.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0186 | Accuracy: 1.0000\nValidation Accuracy: 0.7254\n✅ New best model saved with Val Accuracy: 0.7254\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 97/97 [01:14<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0132 | Accuracy: 1.0000\nValidation Accuracy: 0.7226\n⚠️ No improvement for 1 epoch(s).\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"evaluate_model(densenet161, test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T20:33:38.985790Z","iopub.execute_input":"2025-06-23T20:33:38.986001Z","iopub.status.idle":"2025-06-23T20:33:45.776813Z","shell.execute_reply.started":"2025-06-23T20:33:38.985986Z","shell.execute_reply":"2025-06-23T20:33:45.775918Z"}},"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.7227\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"0.7226993865030675"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"train_model(vit, train_loader, val_loader, epochs=10,save_path = \"best_model_vit\")","metadata":{"id":"rt3mQN8X-SHj","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T21:11:58.155122Z","iopub.execute_input":"2025-06-23T21:11:58.155698Z","iopub.status.idle":"2025-06-23T21:19:44.824509Z","shell.execute_reply.started":"2025-06-23T21:11:58.155674Z","shell.execute_reply":"2025-06-23T21:19:44.823747Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 97/97 [01:39<00:00,  1.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0383 | Accuracy: 0.8862\nValidation Accuracy: 0.4730\n✅ New best model saved with Val Accuracy: 0.4730\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 97/97 [01:47<00:00,  1.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6520 | Accuracy: 0.9599\nValidation Accuracy: 0.4577\n⚠️ No improvement for 1 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 97/97 [01:50<00:00,  1.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3915 | Accuracy: 0.9903\nValidation Accuracy: 0.4508\n⚠️ No improvement for 2 epoch(s).\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 97/97 [01:51<00:00,  1.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2059 | Accuracy: 0.9987\nValidation Accuracy: 0.4563\n⚠️ No improvement for 3 epoch(s).\n🛑 Early stopping triggered.\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"evaluate_model(vit, test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T21:44:22.105071Z","iopub.execute_input":"2025-06-23T21:44:22.105373Z","iopub.status.idle":"2025-06-23T21:44:31.044193Z","shell.execute_reply.started":"2025-06-23T21:44:22.105353Z","shell.execute_reply":"2025-06-23T21:44:31.043463Z"}},"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.4663\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0.4662576687116564"},"metadata":{}}],"execution_count":37}]}